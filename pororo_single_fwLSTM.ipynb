{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1337)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from  torch.nn.utils import clip_grad_norm\n",
    "\n",
    "from bnlstm import LSTM, LSTMCell, BNLSTMCell\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'singleFwLSTM'\n",
    "\n",
    "datapath='/root/data/data_pororo/'\n",
    "savepath='/root/data/save_pororo/'\n",
    "\n",
    "# diagEpisodes: (171,) --- (35,4800), ...\n",
    "#inp = np.load(datapath + 'diagEpisodes.npy')\n",
    "#inp = np.load(datapath + 'pororo_combVec_rebuild.npy') # (16066,9600)\n",
    "inp = np.load(datapath + 'combEmbEpisodes_rebuild.npy') # (171,) --- (35,50), ...\n",
    "stendIdx=np.load(datapath + 'stendIdx.npy')\n",
    "rmIdx=np.load(datapath + 'rmIdx.npy')\n",
    "\n",
    "inp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  17  train:  154  sum: 171\n"
     ]
    }
   ],
   "source": [
    "# inp.shape (171,)\n",
    "# inp[0].shape (36,50)\n",
    "\n",
    "test_idx=[9]\n",
    "X_all=inp\n",
    "\n",
    "test_indices = [i for i in range(len(X_all)) if i % 10 in test_idx]\n",
    "train_indices = [i for i in range(len(X_all)) if i not in test_indices]\n",
    "\n",
    "print( 'test: ',len(test_indices), ' train: ', len(train_indices), \n",
    "                  ' sum:', len(test_indices)+len(train_indices)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "35\n",
      "2\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "X_train = [X_all[i] for i in train_indices]\n",
    "X_test = [X_all[i] for i in test_indices]\n",
    "\n",
    "lenEp=[len(oneEp) for oneEp in X_all]\n",
    "\n",
    "maxLen = 210\n",
    "pairSkVec_train = [zip(oneEp[:-1], oneEp[1:]) for oneEp in X_train]\n",
    "pairSkVec_test = [zip(oneEp[:-1], oneEp[1:]) for oneEp in X_test]\n",
    "#pairSkVec_train = [ (oneEp[:-1], oneEp[1:]) for oneEp in X_train]\n",
    "#pairSkVec_test = [(oneEp[:-1], oneEp[1:]) for oneEp in X_test]\n",
    "\n",
    "i,t=0,0\n",
    "print(len(pairSkVec_train)) #: 154\n",
    "print(len(pairSkVec_train[i])) #: 35 (varible-length) (원래는 36)\n",
    "print(len(pairSkVec_train[i][t])) #: 2\n",
    "print(len(pairSkVec_train[i][t][0])) #: 50\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sequence import pad_sequences\n",
    "X_train_pad = pad_sequences(pairSkVec_train, dtype='float32', padding='pre', truncating='pre', value=0.0)\n",
    "X_test_pad = pad_sequences(pairSkVec_test, dtype='float32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train_pad = nn.utils.rnn.pack_padded_sequence(X_train_pad, lenEp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} #if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader( X_train_pad, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader( X_test_pad, batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D_in=X_train[0].shape\n",
    "D_out=D_in\n",
    "embD=50\n",
    "hidden_size=200\n",
    "num_layers=1\n",
    "use_gpu=True\n",
    "batch_first=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_name='bnlstm'\n",
    "#if model_name == 'bnlstm':\n",
    "#    model = LSTM(cell_class=BNLSTMCell, input_size=D_in,\n",
    "#                 hidden_size=hidden_size, batch_first=batch_first, max_length=maxLen)\n",
    "#elif model_name == 'lstm':\n",
    "#    model = LSTM(cell_class=LSTMCell, input_size=D_in,\n",
    "#                 hidden_size=hidden_size, batch_first=batch_first)\n",
    "#else:\n",
    "#    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, inpD, rnn_inpD, hidden_size, rnn_name='bnlstm', maxLen=250, batch_first=True, bias=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.inpD = inpD\n",
    "        self.rnn_inpD = rnn_inpD\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.fc_in = nn.Linear(inpD, rnn_inpD, bias)\n",
    "        if rnn_name == 'bnlstm':\n",
    "            self.rnn = LSTM(cell_class=BNLSTMCell, input_size=rnn_inpD,\n",
    "                         hidden_size=hidden_size, batch_first=batch_first, max_length=maxLen)\n",
    "        elif rnn_name == 'lstm':\n",
    "            self.rnn = LSTM(cell_class=LSTMCell, input_size=rnn_inpD,\n",
    "                         hidden_size=hidden_size, batch_first=batch_first)\n",
    "        else:\n",
    "            raise ValueError    \n",
    "        self.fc_out = nn.Linear(hidden_size, inpD, bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_rnn_in = F.tanh(self.fc_in(x))\n",
    "        \n",
    "        hx = None\n",
    "\n",
    "        h0 = Variable(x.data.new(x_rnn_in.size(0), hidden_size)\n",
    "                          .normal_(0, 0.1))\n",
    "        c0 = Variable(x.data.new(x_rnn_in.size(0), hidden_size)\n",
    "                          .normal_(0, 0.1))\n",
    "        hx = (h0, c0)\n",
    "        o_n, (h_n, c_n) = self.rnn(input_=x_rnn_in, hx=hx)\n",
    "        if self.batch_first:\n",
    "            o_n = o_n.transpose(0,1)\n",
    "            \n",
    "        out=F.tanh(self.fc_out(o_n))\n",
    "        \n",
    "        return out, x_rnn_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Net(D_in,embD,hidden_size, rnn_name='bnlstm',maxLen=250)\n",
    "\n",
    "#model =torch.nn.DataParallel(model).cuda()\n",
    "#x=x.cuda(async=True)# there is no difference no matter whether we include async=True or not\n",
    "#yt=yt.cuda(async=True)#\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "params = list(model.parameters())\n",
    "#optimizer = optim.RMSprop(params=params, lr=1e-4, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4) \n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    test_loss = 0\n",
    "    for test_batch in test_loader:\n",
    "        test_data = test_batch[:,:,0,:]\n",
    "        test_target = test_batch[:,:,1,:]\n",
    "        \n",
    "        test_data = Variable(test_data, volatile=True)\n",
    "        test_target = Variable(test_target, volatile=True)\n",
    "        if use_gpu:\n",
    "            test_data = test_data.cuda()\n",
    "            test_target = test_target.cuda()\n",
    "\n",
    "        model.train(False)\n",
    "\n",
    "        out, _  = model(test_data)\n",
    "        \n",
    "        test_loss += loss_fn(out, test_target).data[-1]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\t\\tTest set loss: {0:0.6f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logger import Logger  # https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/04-utils/tensorboard/logger.py\n",
    "\n",
    "# Set the logger\n",
    "cur_t = datetime.now()\n",
    "folder_id='{:%Y%m%d_%H%M%S}'.format(cur_t)\n",
    "#log_dir=savepath+'./logs/BiLSTM_LA_emb/'+folder_id + '_s' + str(state_size) + '/'\n",
    "log_dir='./logs/pytorch_emb_story/' + exp_name + '/'+folder_id + '_s' + str(hidden_size) + '/'\n",
    "\n",
    "logger = Logger(log_dir)\n",
    "\n",
    "log_interval = 20\n",
    "save_interval = 10\n",
    "loss_test_hist=[]\n",
    "\n",
    "eidx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\ttrain_loss: 0.045522\n",
      "\t\tTest set loss: 0.008930\n",
      "Epoch: 7\ttrain_loss: 0.043307\n",
      "\t\tTest set loss: 0.008070\n",
      "64.4683749676\n",
      "Epoch: 11\ttrain_loss: 0.040625\n",
      "\t\tTest set loss: 0.006362\n",
      "Epoch: 15\ttrain_loss: 0.043230\n",
      "\t\tTest set loss: 0.004274\n",
      "Epoch: 19\ttrain_loss: 0.040220\n",
      "\t\tTest set loss: 0.004027\n",
      "145.422455788\n",
      "Epoch: 23\ttrain_loss: 0.040594\n",
      "\t\tTest set loss: 0.004227\n",
      "Epoch: 27\ttrain_loss: 0.034258\n",
      "\t\tTest set loss: 0.003843\n"
     ]
    }
   ],
   "source": [
    "iter_cnt=0\n",
    "start_time=time.time()\n",
    "for epoch in range(eidx, eidx+3000):\n",
    "    model.train(True)\n",
    "    for train_batch in train_loader:\n",
    "        #print(train_batch.shape)\n",
    "        train_data = train_batch[:,:,0,:]\n",
    "        train_target = train_batch[:,:,1,:]\n",
    "        #print(train_data.shape, train_target.shape)\n",
    "        train_data = Variable(train_data)\n",
    "        train_target = Variable(train_target)\n",
    "        if use_gpu:\n",
    "            train_data = train_data.cuda()\n",
    "            train_target = train_target.cuda()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        out, _ =model(train_data)\n",
    "        \n",
    "        train_loss = loss_fn(out, train_target)\n",
    "        train_loss.backward()\n",
    "        clip_grad_norm(parameters=params, max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter_cnt % log_interval == 0:\n",
    "            print(\"Epoch: {0}\\ttrain_loss: {1:0.6f}\".format(epoch, train_loss.data[0]) )\n",
    "            \n",
    "            loss_test_hist.append(test(model,test_loader))\n",
    "\n",
    "            #============ TensorBoard logging ============#\n",
    "            # (1) Log the scalar values\n",
    "            info = {\n",
    "                'loss': train_loss.data[0],\n",
    "                'val_loss': loss_test_hist[-1]\n",
    "            }\n",
    "\n",
    "            for tag, value in info.items():\n",
    "                logger.scalar_summary(tag, value, epoch+1)\n",
    "\n",
    "            # (2) Log values and gradients of the parameters (histogram)\n",
    "            for tag, value in model.named_parameters():\n",
    "                tag = tag.replace('.', '/')\n",
    "                logger.histo_summary(tag, to_np(value), epoch+1)\n",
    "                logger.histo_summary(tag+'/grad', to_np(value.grad), epoch+1)\n",
    "            #=============================================#\n",
    "            \n",
    "        iter_cnt += 1\n",
    "    if epoch % save_interval == 0:\n",
    "        save_filename = '{}/{}_h{}_epoch{}'.format(savepath, exp_name, hidden_size, epoch)\n",
    "        torch.save(model, save_filename)\n",
    "        print('saving.... by now Elapsed time: ', time.time() - start_time)\n",
    "    eidx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class simpleFWLSTM(nn.Module):\n",
    "    def __init__(self, D_in, lstm_dim, num_layers=1,\n",
    "                use_bias=True, batch_first=False, dropout=0, **kwargs):\n",
    "        super(simpleFWLSTM, self).__init__()\n",
    "        self.D_in = D_in\n",
    "        self.lsmt_dim = lstm_dim\n",
    "        self.embD = embD\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = nn.Linear(D_in,embD)\n",
    "        self.lstm = nn.LSTMCell(embD, lstm_dim)\n",
    "        self.decoder = nn.Linear(lstm_dim, D_in)\n",
    "        \n",
    "    def forward(self, x, future = 0):\n",
    "        \n",
    "        outputs = []\n",
    "        embVec = self.encoder(x, self.embD)\n",
    "        hx,cx = self.lstm(embVec, self.lstm_dim)\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            h_t, c_t = self.lstm(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(c_t, (h_t2, c_t2))\n",
    "            outputs += [c_t2]\n",
    "        #for i in range(future):# if we should predict the future\n",
    "        #    h_t, c_t = self.lstm1(c_t2, (h_t, c_t))\n",
    "        #    h_t2, c_t2 = self.lstm2(c_t, (h_t2, c_t2))\n",
    "        #    outputs += [c_t2]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
